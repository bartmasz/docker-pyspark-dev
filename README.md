# Docker PySpark Development Environment

| Work in progress

Objective of this repository is to build environment with:
 * PySpark
 * Airflow
 * JupyterLab

 that will allow designing data pipelines.

 ## How To Use It?

 1. Execute bash scripts in root directory.
 1. Login to airlow and enable `a_sparkoperator_test`.
